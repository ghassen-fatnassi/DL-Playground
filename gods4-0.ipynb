{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10760180,"sourceType":"datasetVersion","datasetId":6674516}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install torch transformers datasets tqdm scikit-learn accelerate imbalanced-learn joblib","metadata":{"_uuid":"95b5d773-6d3a-4ec8-9332-cc16f16dbf59","_cell_guid":"71bff9cf-36a3-4339-bac0-bb5ab895c9b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T23:56:18.164814Z","iopub.execute_input":"2025-02-16T23:56:18.165132Z","iopub.status.idle":"2025-02-16T23:56:24.089732Z","shell.execute_reply.started":"2025-02-16T23:56:18.165101Z","shell.execute_reply":"2025-02-16T23:56:24.088786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom datasets import Dataset as HFDataset\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom accelerate import Accelerator, notebook_launcher\nfrom sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nimport torch.nn.functional as F\nimport joblib","metadata":{"_uuid":"3864079c-30e7-469f-8ab9-34da9cf1b133","_cell_guid":"7025a374-1b69-438e-833a-93006fc96a3f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T23:56:24.090759Z","iopub.execute_input":"2025-02-16T23:56:24.090992Z","iopub.status.idle":"2025-02-16T23:56:58.121199Z","shell.execute_reply.started":"2025-02-16T23:56:24.090965Z","shell.execute_reply":"2025-02-16T23:56:58.120544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass Config:\n    content_length_percentile = 90\n    min_text_length = 0\n    train_on_subset = False\n    subset_fraction = 0.01\n    \n    pretrained_model = \"allenai/longformer-base-4096\"\n    dropout_rate = 0.2\n    max_length = None\n    \n    batch_size = 4\n    num_epochs = 4\n    learning_rate = 2e-5\n    weight_decay = 0.01\n    warmup_ratio = 0.1\n    \n    mixed_precision = 'fp16'\n    seed = 428\n    gradient_accumulation_steps = 8\n\nconfig = Config()","metadata":{"_uuid":"2b18a0ba-cc3a-405d-a09b-9490a1ec43ed","_cell_guid":"1bf4bfd7-719f-4475-a535-03bcab828352","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T23:56:58.122468Z","iopub.execute_input":"2025-02-16T23:56:58.122976Z","iopub.status.idle":"2025-02-16T23:56:58.127211Z","shell.execute_reply.started":"2025-02-16T23:56:58.122952Z","shell.execute_reply":"2025-02-16T23:56:58.126490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntorch.manual_seed(config.seed)\nnp.random.seed(config.seed)\n\ndef clean_data(df):\n    df = df.dropna(subset=['content']).copy()\n    df['text_length'] = df['content'].apply(lambda x: len(str(x).split()))\n    length_threshold = np.percentile(df['text_length'], config.content_length_percentile)\n    df = df[(df['text_length'] >= config.min_text_length) & \n            (df['text_length'] <= length_threshold)]\n    \n    config.max_length = int(np.ceil(length_threshold / 2048) * 2048)\n    print(f\"Using dynamic max length: {config.max_length}\")\n    \n    return df","metadata":{"_uuid":"7845214e-56de-4895-901e-f32c0aa0bebd","_cell_guid":"96041ef7-9994-4815-bdd5-00d1f339c756","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T23:56:58.128485Z","iopub.execute_input":"2025-02-16T23:56:58.128887Z","iopub.status.idle":"2025-02-16T23:56:58.163178Z","shell.execute_reply.started":"2025-02-16T23:56:58.128813Z","shell.execute_reply":"2025-02-16T23:56:58.162585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf_train = pd.read_csv(\"/kaggle/input/dataset-gods/train.csv\")\ndf_train = clean_data(df_train)\n\ndf_train['text'] = df_train['title'] + \" \" + df_train['content']\n\nlabel_encoder = LabelEncoder()\ndf_train[\"target\"] = label_encoder.fit_transform(df_train[\"target\"])\nnum_classes = len(label_encoder.classes_)\n\nrus = RandomUnderSampler(random_state=config.seed)\ndf_resampled, _ = rus.fit_resample(df_train[['text', 'text_length']], df_train['target'])\ndf_train = pd.DataFrame({\n    'text': df_resampled['text'],\n    'target': df_resampled.index.map(df_train['target'].__getitem__)\n})\n\nif config.train_on_subset:\n    df_train = df_train.sample(frac=config.subset_fraction, random_state=config.seed)\n\nfrom sklearn.model_selection import train_test_split\ndf_train, df_val = train_test_split(df_train, test_size=0.2, random_state=config.seed)","metadata":{"_uuid":"288fa123-d6c9-48d3-86e6-581b858d9eb8","_cell_guid":"6e5d9128-723b-45db-a1ef-1673a10ea337","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T23:56:58.163989Z","iopub.execute_input":"2025-02-16T23:56:58.164255Z","iopub.status.idle":"2025-02-16T23:57:01.576670Z","shell.execute_reply.started":"2025-02-16T23:56:58.164216Z","shell.execute_reply":"2025-02-16T23:57:01.575756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass MentalHealthDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts.iloc[idx])\n        label = self.labels.iloc[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=config.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"_uuid":"fc4dfbec-0373-4c4a-90d0-f1f620b6a65a","_cell_guid":"47bf3890-00cc-4799-b117-5d86312f7ecd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T23:57:01.577565Z","iopub.execute_input":"2025-02-16T23:57:01.577791Z","iopub.status.idle":"2025-02-16T23:57:01.583710Z","shell.execute_reply.started":"2025-02-16T23:57:01.577772Z","shell.execute_reply":"2025-02-16T23:57:01.582740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntokenizer = LongformerTokenizer.from_pretrained(config.pretrained_model)\n\ndef initialize_model(num_classes):\n    model = LongformerForSequenceClassification.from_pretrained(\n        config.pretrained_model,\n        num_labels=num_classes,\n    )\n    return model","metadata":{"_uuid":"ac97ebc4-4502-490e-acdb-6d69d3efb803","_cell_guid":"6de59507-ea83-405c-994f-a1f6ec09387d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T23:57:01.584673Z","iopub.execute_input":"2025-02-16T23:57:01.585030Z","iopub.status.idle":"2025-02-16T23:57:03.862434Z","shell.execute_reply.started":"2025-02-16T23:57:01.584996Z","shell.execute_reply":"2025-02-16T23:57:03.861749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef training_function():\n    accelerator = Accelerator(mixed_precision=config.mixed_precision, gradient_accumulation_steps=config.gradient_accumulation_steps)\n    \n    model = initialize_model(num_classes)\n\n    optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n    \n    train_dataset = MentalHealthDataset(df_train['text'], df_train['target'], tokenizer)\n    val_dataset = MentalHealthDataset(df_val['text'], df_val['target'], tokenizer)\n    \n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n\n    total_steps = len(train_loader) * config.num_epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=int(total_steps * config.warmup_ratio),\n        num_training_steps=total_steps\n    )\n\n    class_counts = np.bincount(df_train['target'])\n    class_weights = torch.tensor(1. / np.sqrt(class_counts), dtype=torch.float32)\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(accelerator.device))\n\n    model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(\n        model, optimizer, train_loader, val_loader, scheduler\n    )\n\n    def evaluate(model, loader):\n        model.eval()\n        all_preds = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch in tqdm(loader, desc=\"Evaluating\"):\n                outputs = model(**batch)\n                preds = torch.argmax(outputs.logits, dim=-1)\n                \n                all_preds.extend(accelerator.gather_for_metrics(preds).cpu())\n                all_labels.extend(accelerator.gather_for_metrics(batch['labels']).cpu())\n        \n        accuracy = accuracy_score(all_labels, all_preds)\n        f1 = f1_score(all_labels, all_preds, average='weighted')\n        return accuracy, f1\n\n    best_f1 = 0\n    for epoch in range(config.num_epochs):\n        model.train()\n        total_loss = 0\n        all_preds = []\n        all_labels = []\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\", leave=False)\n        for batch in progress_bar:\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = criterion(outputs.logits, batch['labels'])\n                \n                accelerator.backward(loss)\n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n                    \n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                \n                total_loss += loss.item()\n                preds = torch.argmax(outputs.logits, dim=-1)\n                all_preds.extend(accelerator.gather_for_metrics(preds).cpu())\n                all_labels.extend(accelerator.gather_for_metrics(batch['labels']).cpu())\n                \n                progress_bar.set_postfix(loss=total_loss / len(all_labels))\n\n        train_accuracy, train_f1 = evaluate(model, train_loader)\n        val_accuracy, val_f1 = evaluate(model, val_loader)\n        print(f\"Epoch {epoch+1}:\")\n        print(f\"  Train Accuracy: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n        print(f\"  Validation Accuracy: {val_accuracy:.4f}, Validation F1: {val_f1:.4f}\")\n        \n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            print(\"Saving model with best validation F1 score...\")\n            accelerator.save_state(\"best_model\")\n\n    return model","metadata":{"_uuid":"5b3be96e-4f94-43a6-93ab-b1d0de7b8735","_cell_guid":"fd934679-2c37-4421-92c1-a52fe04dba1f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T23:57:03.864493Z","iopub.execute_input":"2025-02-16T23:57:03.864726Z","iopub.status.idle":"2025-02-16T23:57:03.876068Z","shell.execute_reply.started":"2025-02-16T23:57:03.864707Z","shell.execute_reply":"2025-02-16T23:57:03.875314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    notebook_launcher(training_function, num_processes=1)","metadata":{"_uuid":"8c7d4f9e-ed8d-4010-98c0-bc76b1f9c526","_cell_guid":"a26f2105-8751-4874-8fe5-9b018285583e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T23:57:03.877314Z","iopub.execute_input":"2025-02-16T23:57:03.877516Z","iopub.status.idle":"2025-02-16T23:57:03.887105Z","shell.execute_reply.started":"2025-02-16T23:57:03.877485Z","shell.execute_reply":"2025-02-16T23:57:03.886343Z"}},"outputs":[],"execution_count":null}]}